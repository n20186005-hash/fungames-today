import json
import requests
import feedparser # ä¸“é—¨ç”¨æ¥è§£ææ–°é—»RSSçš„åº“
import datetime
import os
import time
import subprocess
import traceback

# --- é…ç½®éƒ¨åˆ† ---
GAME_FEED_URL = "https://gamemonetize.com/rss.php?format=json"
# è¿™é‡Œä½¿ç”¨ IGN çš„æ¸¸æˆæ–°é—»æº (ä½ ä¹Ÿå¯ä»¥æ¢æˆ GameSpot æˆ– Kotaku)
NEWS_RSS_URL = "https://feeds.ign.com/ign/news"
BASE_DIR = "fungames.today/MyGameSite"

# --- 1. æ¸¸æˆæ›´æ–°æ¨¡å—ï¼ˆå·²ä¿®æ”¹ï¼šæ”¹ä¸ºåˆå¹¶+å»é‡ï¼Œå¹¶ä¸ºæ–°æ¡ç›®æ·»åŠ  added_atï¼‰ ---
def update_games():
    print("ğŸ® æ­£åœ¨æŠ“å–æ–°æ¸¸æˆ...")
    data_dir = os.path.join(BASE_DIR, 'data')
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    headers = { 'User-Agent': 'Mozilla/5.0' }
    try:
        response = requests.get(GAME_FEED_URL, headers=headers, timeout=15)
        if response.status_code == 200:
            # å°è¯•è§£æå‡ºä¸€ä¸ªåˆ—è¡¨ï¼ˆåŸæ¥æ˜¯ç›´æ¥ response.json()[:20] è¦†ç›–ï¼‰
            resp_json = response.json()
            # å¦‚æœè¿”å›çš„æ˜¯ dict ä¸”åŒ…å«åˆ—è¡¨å­—æ®µï¼Œå°½é‡è·å–åˆ—è¡¨
            new_games_raw = []
            if isinstance(resp_json, list):
                new_games_raw = resp_json
            elif isinstance(resp_json, dict):
                # å¸¸è§å­—æ®µåå°è¯•
                for k in ('items', 'data', 'results', 'games', 'rows', 'feed'):
                    if k in resp_json and isinstance(resp_json[k], list):
                        new_games_raw = resp_json[k]
                        break
                # fallback: æŠŠ dict å½“ä½œå•ä¸ªå…ƒç´ 
                if not new_games_raw:
                    # å¦‚æœ dict é‡Œæœ‰åˆ—è¡¨å­—æ®µï¼Œå–ç¬¬ä¸€ä¸ªåˆ—è¡¨
                    for v in resp_json.values():
                        if isinstance(v, list):
                            new_games_raw = v
                            break
                    if not new_games_raw:
                        # æœ€åé€€å›æŠŠæ•´ä¸ª dict å½“ä½œå•é¡¹
                        new_games_raw = [resp_json]
            else:
                new_games_raw = []

            # åªå–å‰ 50 æ¡ä»¥é˜²è¿‡å¤§ï¼ˆéœ€è¦æ—¶å¯è°ƒæ•´ï¼‰
            new_games_raw = new_games_raw[:50]

            # è§„èŒƒåŒ–æ¡ç›®ï¼šå°½é‡å– url/title/thumbnail/description/category
            def normalize(raw):
                if not isinstance(raw, dict):
                    return None
                url = None
                for key in ('url','link','game_url','gameLink','page','href'):
                    if key in raw and raw[key]:
                        url = raw[key]
                        break
                # å°è¯•åµŒå¥—å­—æ®µ
                if not url:
                    for nk in ('data','item','attributes'):
                        if nk in raw and isinstance(raw[nk], dict):
                            for key in ('url','link'):
                                if key in raw[nk] and raw[nk][key]:
                                    url = raw[nk][key]
                                    break
                            if url:
                                break
                if not url:
                    return None
                title = raw.get('title') or raw.get('name') or url
                thumbnail = raw.get('thumbnail') or raw.get('thumb') or raw.get('image') or ''
                description = raw.get('description') or raw.get('desc') or ''
                category = raw.get('category') or raw.get('tag') or ''
                return {
                    'url': url,
                    'title': title,
                    'thumbnail': thumbnail,
                    'description': description,
                    'category': category
                }

            fetched = []
            for r in new_games_raw:
                n = normalize(r)
                if n:
                    fetched.append(n)

            games_file = os.path.join(data_dir, 'games.json')
            existing = []
            if os.path.exists(games_file):
                try:
                    with open(games_file, 'r', encoding='utf-8') as f:
                        existing = json.load(f)
                except Exception as e:
                    print("âš ï¸ è¯»å–å·²æœ‰ games.json å¤±è´¥ï¼Œå°†ä»ç©ºåˆ—è¡¨å¼€å§‹:", e)

            existing_map = { item.get('url'): item for item in existing if item.get('url') }
            added = 0
            for g in fetched:
                url = g['url']
                if url in existing_map:
                    # å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼ˆè‹¥ä½ æƒ³æ›´æ–°å­—æ®µå¯åœ¨æ­¤åˆå¹¶ï¼‰
                    continue
                entry = {
                    'url': url,
                    'title': g.get('title') or url,
                    'description': g.get('description',''),
                    'thumbnail': g.get('thumbnail',''),
                    'category': g.get('category',''),
                    'added_at': datetime.datetime.utcnow().isoformat() + 'Z'
                }
                existing.append(entry)
                existing_map[url] = entry
                added += 1

            # æŒ‰æ—¶é—´é™åºä¿å­˜
            existing.sort(key=lambda x: x.get('added_at',''), reverse=True)
            with open(games_file, 'w', encoding='utf-8') as f:
                json.dump(existing, f, ensure_ascii=False, indent=2)

            print(f"âœ… æ¸¸æˆæ›´æ–°å®Œæˆï¼ˆåˆå¹¶å†™å…¥ï¼‰ï¼Œæ–°å¢ {added} æ¡")
            return added
        else:
            print(f"âŒ è¯·æ±‚æ¸¸æˆæºå¤±è´¥ï¼ŒçŠ¶æ€ç  {response.status_code}")
            return 0
    except Exception as e:
        print("âŒ æ¸¸æˆæ›´æ–°å¤±è´¥:", e)
        traceback.print_exc()
        return 0

# --- 2. æ–°é—»æ›´æ–°æ¨¡å— (åŸæ ·ä¿ç•™ï¼šæœªåšæ”¹åŠ¨) ---
def update_news():
    print("ğŸ“° æ­£åœ¨æŠ“å–æ–°é—»...")
    data_dir = os.path.join(BASE_DIR, 'data')
    news_file = os.path.join(data_dir, 'news.json')
    
    # A. å…ˆè¯»å–æ—§æ–°é—»ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    existing_news = []
    if os.path.exists(news_file):
        try:
            with open(news_file, 'r', encoding='utf-8') as f:
                existing_news = json.load(f)
        except:
            existing_news = []
    
    # B. æŠ“å–æ–°æ–°é—»
    try:
        feed = feedparser.parse(NEWS_RSS_URL)
        new_items = []
        
        # å»ºç«‹ä¸€ä¸ªæ—§æ ‡é¢˜çš„é›†åˆï¼Œç”¨æ¥æŸ¥é‡
        existing_titles = {item['title'] for item in existing_news}
        
        for entry in feed.entries:
            # å¦‚æœè¿™ç¯‡æ–°é—»ä¹‹å‰æ²¡å­˜è¿‡ï¼Œæ‰æ·»åŠ 
            if entry.title not in existing_titles:
                # æå–å›¾ç‰‡ (RSSé‡Œå›¾ç‰‡é€šå¸¸åœ¨ media_content æˆ– summary é‡Œï¼Œè¿™é‡Œåšä¸ªç®€å•å¤„ç†)
                image_url = ""
                if 'media_content' in entry:
                    image_url = entry.media_content[0]['url']
                
                news_item = {
                    "title": entry.title,
                    "date": datetime.datetime.now().strftime("%Y-%m-%d"), # è®°å½•ä»Šå¤©æ—¥æœŸ
                    "desc": entry.summary[:150] + "...", # åªå–å‰150ä¸ªå­—
                    "tag": "News", # é»˜è®¤æ ‡ç­¾
                    "source": "IGN", # æ¥æº
                    "link": entry.link # åŸæ–‡é“¾æ¥
                }
                new_items.append(news_item)
        
        if new_items:
            # C. æŠŠæ–°æ–°é—»åŠ åˆ°æœ€å‰é¢ (Prepend)
            final_news = new_items + existing_news
            
            # ä¸ºäº†é˜²æ­¢æ–‡ä»¶æ— é™å¤§ï¼Œæˆ‘ä»¬å¯ä»¥åªä¿ç•™æœ€è¿‘ 100 æ¡ (å¯é€‰)
            final_news = final_news[:100]
            
            with open(news_file, 'w', encoding='utf-8') as f:
                json.dump(final_news, f, ensure_ascii=False, indent=2)
            print(f"âœ… æ–°å¢äº† {len(new_items)} æ¡æ–°é—»ï¼")
            return len(new_items)
        else:
            print("ğŸ’¤ æ²¡æœ‰å‘ç°æ–°æ–°é—»ã€‚")
            return 0
            
    except Exception as e:
        print(f"âŒ æ–°é—»æ›´æ–°å¤±è´¥: {e}")
        return 0

# --- ä¸»ç¨‹åºï¼šä¿æŒä½ åŸæœ¬é€»è¾‘å†™ metaï¼Œä½†æ˜¯ç°åœ¨ä½¿ç”¨åˆå¹¶åçš„æ¸¸æˆæ•°é‡å’Œæ–°é—»æ•°é‡ ---
if __name__ == "__main__":
    try:
        games_added = update_games()
        news_added = update_news()
    except Exception as e:
        print("âŒ æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸:", e)
        games_added = 0
        news_added = 0

    # ç¡®ä¿ data ç›®å½•å­˜åœ¨ï¼Œç„¶åå†™ meta.jsonï¼ˆå’Œä½ åŸæ¥å†™ meta çš„ä¿¡æ¯ç›¸åŒï¼‰
    meta_dir = os.path.join(BASE_DIR, 'data')
    if not os.path.exists(meta_dir):
        os.makedirs(meta_dir)

    meta_data = {
        "last_update": datetime.datetime.now().strftime("%Y-%m-%d"),
        "new_count": games_added,
        "news_count": news_added, # è®°å½•æ–°é—»æ•°é‡
        "notification": f"Update: {games_added} Games & {news_added} News Added!"
    }

    meta_path = os.path.join(meta_dir, 'meta.json')
    with open(meta_path, 'w', encoding='utf-8') as f:
        json.dump(meta_data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ”” å†™å…¥ meta: {meta_path}")

    # å¦‚æœæœ‰æ–°å¢ï¼Œåˆ™å°è¯•æäº¤å¹¶æ¨é€ï¼ˆåœ¨ Actions ä¸­éœ€è¦ permissions: contents: writeï¼‰
    if games_added > 0 or news_added > 0:
        try:
            subprocess.run(['git', 'config', 'user.email', 'actions@users.noreply.github.com'], check=True)
            subprocess.run(['git', 'config', 'user.name', 'github-actions'], check=True)
            paths = [os.path.join(BASE_DIR, 'data', 'games.json'),
                     os.path.join(BASE_DIR, 'data', 'news.json'),
                     os.path.join(BASE_DIR, 'data', 'meta.json')]
            subprocess.run(['git', 'add'] + paths, check=True)
            subprocess.run(['git', 'commit', '-m', f'Auto update: +{games_added} games +{news_added} news'], check=False)
            subprocess.run(['git', 'push'], check=False)
            print("âœ… å°è¯•æ‰§è¡Œ git pushï¼ˆå¦‚æœ workflow æœ‰æƒé™åˆ™ä¼šæˆåŠŸï¼‰")
        except Exception as e:
            print("âš ï¸ å°è¯• git push å¤±è´¥ï¼š", e)
            traceback.print_exc()
    else:
        print("â„¹ï¸ æœ¬æ¬¡æ— æ–°å¢å†…å®¹ï¼Œè·³è¿‡æäº¤ã€‚")
